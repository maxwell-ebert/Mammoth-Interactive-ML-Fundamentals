## Data Transformation For Machine Learning

1. Performance of a Machine Learning Algorithm
	- Steps
		1. Collect dataset
		2. Divide into training and testing sets
		3. Generate h(x) with training set
		4. Measure % of examples of the testing set correctly classified h(x)
	- Things to watch out for:
		1. ENSURE that training and testing sets are separate
		2. An algorithm will remember testing sets
			- DO NOT change the algorithm after testing and then re-test on the same testing set
			- Possible to just re-build algorithm in a different file and/or object
			- Don't run algorithm twice on same testing set

2. Handle Noise in Data
	- Noise 
		- Random errors in learning data
	- How does noise affect machine learning?
		- Can lead to Overfitting
			- Overfitting: Larger trees make more mistakes on new data
				- ML model ingests noise and makes decisions based on noise
		- We must reduce noise in ML data
	- How can you handle noise?
		1. Split training set into 70% learning and 30% validation
		2. Build model on learning set
		3. Iterate as long as accuracy on validation set increases
			- As you're training, you're checking
			- Is accuracy on my validation still increasing?
				- As long as accuracy is increasing, then can continue to train ML model
				- If accuracy on validation starts to decrease, then time to stop training
					- The model is starting to eat noise
	- Why does learning work?
		- Weigh attributes to build as shallow a decision tree as possible
		- Weight are calculated based on information gain
		- Starts with most attribute getting most weight
			- Least important attributes get little or no weight

3. Powerful Tools with Machine Learning Libraries
	- What is a library?
		- Resources used by computer programs, often for software dev
			- eg) configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications
	- What a library can provide:
		- Data
		- ML Models
		- Functionality
		- Pre-Written code
		- and more!
	- One example of popular ML library is Keras
		- Neural network library in Python built on TensorFlow
			- TensorFlow is google's ML kit
		- Allows fast experimentation on deep neural networks
	- TensorFlow 2
		- An open-source software library
		- Developed by Google 
		- Lots of research/funding as a Google-developed tool
		- Its math library is often used for neural networks
		- TensorFlow 2 offers easy model building
			- Templated ML Model building
	- TensorFlow.js
		- Machine Learning library for JavaScript
		- Available in web browser
		- Also on Node.js
		- Why TensorFlow.js?
			- Access device features (camera)
			- Add ML to JS apps
			- Can import TensorFlow Python Models for use in JS
	- TensorFlow vs TensorFlow.js	
		- TensorFlow came first (2015)
		- Typically used with Python
		- Partially implemented in native C++ code
		- TensorFlow cannot run in browser
		- TensorFlow.js provides the same functionality with JS
		- Both are both open source
		- Both are machine learning libraries
		- Both are made by Google
		- Both have Similar APIs
		- JS is not as fast as native code
		- Deep learning is possible: TF.js is fast
		- Optimized backends in TF.js
		- Uses GPU in browser
		- Uses native bindings with Node.js
	- Most common ML libraries:
		1. Pandas
			- Commonly used for ETL work
			- Open source data analysis and manipulation tools
			- Built on top of Python
			- Provides fast, flexible and expressive data structures
			- Designed to make working with data easy and intuitive
			- Good to use with:
				- SQL Table or excel spreadsheet
				- Time series data
				- Matrix data
				- Observational/Statistical datasets
			- Things you can do with Pandas:
				- Handle missing data
				- Mutate size
				- Align data
				- Convert data into DataFrame objects
				- Join datasets
				- Pivot data sets
				- Time series functionality
		2. NumPy
			- Commonly used for data evaluation
			- Allows for scientific computing with Python
			- Powerful n-dimension arrays
			- Mathematical functions and more numerical computing tools
		3. Matplotlib
			- Commonly used for Data Exploration 
			- Python 2D plotting library
			- Generate plots, histograms, bar charts, scatterplots and more with a few lines of code
		4. SkiKit-Learn
			- Commonly used for Data Modeling
			- Lots of ML templates, and ML-related tools
			- Simple/efficient tool for predictive data analysis 
			- Capable of:
				- Classification
				- Regression
				- Clustering
				- Dimensionality reduction
				- Model Selection
				- Preprocessing
	- Python has many visualization libraries
		- Matplotlib, pandas
		- Many are static
			- Plotly allows for building of interactive images
				- Open source library
				- General data viz library
				- Interactive visualization
				- Libraries for JavaScript, React, R, and Python
					- Python version is most popular
				- Using Plotly alone builds interactive plots as .html files
					- Users can zoom in, select, hover, etc
					- Plots cannot connect to changing data sources
					- Data is locked into exported state
					- Need to re-run the python script and re-generate the .html file to see updates
			- Dash allows for creation of dashboard web apps
				- Plots can interact with each other
				- Plots can Interact with components
				- Plots Update in real time
				- Open source library from plotly that builds full dashboard
				- Instead of .html file, Dash builds a dashboard web app at a Local URL
					- 127.0.0.1:8050
				- Can visit and interact dashboard in webapp
			- TF Lite
				- Run ML models on mobile, embedded, and IoT devices
					- Data can stay on device
					- Can send training results back to server
					- Light on processing load
					- Open source deep learning framework for on-device inference
					- Some features:
						- Optimized for on-device machine learning
						- Latency (no round-trip to a server)
						- Privacy (no personal data leaves device)
						- connectivity (internet connectivity not required)
						- size (reduced model + binary size)
						- Power consumption (efficient inference + lack of network connections)
						- Multiple platform support (Android, iOS, Linus, microcontrollers)
						- Supports many languages (Java, Swift, Objective-C, C++, Python...)
						- High performance (hardware acceleration + model optimization)
						- https://www.tensorflow.org/lite/guide

4. Load and Clean A Public Dataset
	- We will be using data from UCI
		- found at: https://archive.ics.uci.edu/ml/datasets/adult
		- note prediction task, attributes, and other features
		- While source data will be saved in my repo, it needs to also be saved to google drive in order to use in Colab
		
5. What is One Hot Encoding?
	- uses categorical data	
		- To refresh: data that has label values rather than numerical (eg) dogs and cats dataset)
		- Categorical data can be referred to as Nominal Data
		- Some models like decision tree can train from categorical data without requiring any data transformation
		- Most ML cannot read english, and can only understand numbers
			- To compensate, encode each categorical value to a numeric value
	- Integer Encoding
		- also called Ordinal Enconding or Label Encoding	
			- imagine the Temperature Dataset: Col 1 is id and [0,1,2,3] col 2 is temperature [cold, warm, hot, warm]
				- Converting temperature into integers would give us [1,2,3,2] where 1 = cold, 2 = warm, and 3 = hot
		- Can only use integer encoding in cases where values have a natural ordered relationship with one another
	- In One-Hot encoding, the integer encoded variable is removed
	- new binary variable is added for each unique integer value
		- example, datset of dogs and cats
			- can't use integer encoding because dogs/cats do not have an ordered relationship
			- instead, create 2 columns: animal_dog and animal_cat, and each is assigned either 0 or 1 based on what animal it is	
				- For a dog, animal_dog = 1 and animal_cat = 0, and vice-versa
				- can create as many one hot encoded categories as there are possible categories
	- In one hot encoding, each output becomes an object to be rated by the model
		- common method of preprocessing categorical features
		- make a binary feature for each category
		- Assign 1 to the feature of each sample that contains it
	- Learned embedding
		- inbetween one hot and integer encoding
			- allows similar words to be converted to the same numeric representation so that ML model can learn their similarity
			- allows classes to be represented as vector 
			
6. Build X and Y Data with One Hot Encoding
	- see google colab "load and clean a public dataset"
	
7. Logistic Regression With One Hot Encoding	
	- see google colab "load and clean a public dataset"
	
8. Scale  and encode data with SciKit-Learn
	- see google colab "load and clean a public dataset"
	
9. What is Scaling Data?
	- Consider dataset with 4 features(columns) and 5 values (rows)
	- There will be outliers
	- feature3 has data that lies in the .00000000001 range
	- if there are broad differences in features, then features must be scaled so that ML Model doesn't ignore
	- if features in a dataset have values on different scales, then this can cause problems
	- Scaling changes the range of values
		- the shape of the distibution stays the same
		- eg) scale model of a city has the same proportions
	- feature3 is now scaled to match other features
		- Typically, features are scaled on a range of 0 to 1
		- consider two charts:
			- the original has 0 to 50 on the x axis, and 1 to 10 on the y axis
			- the scaled data has 0 to 1 on the x axis, and 50 to 500 on the y axis
				- because we scaled the 0-50 range of the first chart to 0 to 1, we must do the same (multiply by 50) to the y axis
				- This preserves distribution
	- SciKit-Learn provides MinMaxScaler!
		- The default range returned by MinMaxScaler is 0 to 1
		- The relative spaces between each feature's values are saved
		- MinMaxScaler doesn't reduce the importance of outliers
		- Use other scalers if you want a normal distribution or for outliers to have less influence
			
10. Build, Train and Test A Machine Learning Model
	- see google colab

11. Compare Decision Tree and Linear Regression Models
	- see google colab

12. What is the KBins Discretizer?
	- Discretization
	- Required to use if we have numerical data with highly skewed or nonstandard distribution	
		- Many outliers, or has multi-modal distribution 
			- or exponential distribution where common objects are bunched together
	- Algorithims perform better when numerical variables have a standard probability distibution
	- Some algorithms prefer or require categorical or ordinal input variables
	- We can use discretizers to improve algorithms	
		- Translating a quantitative variable into 2 ore more qualitative buckets or categories	
		- essentially changing a numeric variable into a categorical 
		- opposite of one hot encoding
		- also called binning
			- bins are assigned a number
	- KBins is one type of discretization
		- Bins refer to grouping, K refers to number of groups 
		- values for a var are grouped into discrete bins
		- ordinal relationship between bins are preserved
		- consider column temperature on a given day in a dataset
			- temp can range from 0-100
			- you can bin into groups of 0-10, 10-20, etc
			- now have converted numerical data into binned data
	- KBins Code:
	EXAMPLE 1:
	from sklearn.preprocessing import KBinsDiscretizer
	
	discretizer = KBinsDiscretizer(n_bins = 10, strategy = 'uniform')
	
	discretizer.fit(X)
	
	X_binned = discretizer.transform(X)
	
	EXAMPLE 2:
	encoding_discretizer = KBinsDiscretizer(n_bins = 10,
											strategy = "Uniform"
											encode = "onehot-dense")
	encoding_discretizer.fit(X)
	
	X_binned = encoding_discretizer.transform(X)
	
	strategies: uniform (flexible number of bins), quantized (number of bins less than number of observations), kmeans (must use value for number of clusters that can be reasonably found)
	
	typically use ordinal encoding in KBins
	
13. Bin Data with KBins Discretizer
	- See colab 
	