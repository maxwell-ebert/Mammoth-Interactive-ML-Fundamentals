#Session 2 - Machine Learning Fundamentals - 1/4/2023

1. Probability And Statistics For Machine Learning
a. Probability and Information Theory Overview
	- Probability refers to the ability to quantify uncertainty
	- ML models are trained on imperfect data
		-Real world data is messy, with outliers, noise, etc
	- Understanding probability will help you better understand what your ML model is doing and how it works. 
		- Can understand appropriate var types and probability distributions	
		- Can understand standard techniques for assessing relationships between distributions
			- As well as use probability/information theory to quantify viable data among noise 
	- Coin flip - a fair coin is equally likely to come up heads as it is to come up tails. 
		- can use probability to express the likelihood of an event by comparing it to the sample space. 
	- Event-probability equation 
		- P(event)=(# of outcomes of event)/(# of outcomes in  Ω) where  Ω = sample space
			- probability of coin flips = number of events (1, heads) divided by 2 (coin can come up heads/tails) (50% probability or 0.5)
				-  Ω = {H,T}
				- P(H) = .5
				- P(T) = .5
	- Multiple Independent Observations
		- eg) 2 observations
			-  Ω = {HH, HT, TH, TT} (Total possible outcomes are now 4 versus just heads/tails)
			- P(HH)= .25, etc
		- eg) 3 observations
			-  Ω = {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}
			- P(HHH) = .13, etc
	- Combining Probability (Combinatorics)
		- To combine probabilities, multiply them
			eg) P(HHHHH) = P(HH) * P(HHH)=1/4 * 1/8 = 1/32 = .03 (still using coin flips in this example)
		- Combinatorics is field of mathematics devoted to counting
		- Use factorials to calculate probabilities	
			- Number of combinations equation:
				- (n  k) = n!/k!(n-k)! where n = # of observations and k = event 
				- the number of observations of an event is equal to the numbers of observations factorial over the event factorial multiplied by (n minus k)factorial
			- eg) find number of ways to get 2 head flips in 3 coin flips 
				- n = 3 coin flips
				- k = 2 head flips
					-Using number of combinations equation:
						(3  2) = 3!/2!(3-2)! = 3
				- Result
					- Number of combinations gave us first part of info for P(event) equation 
						- Numerator for the event-probability equation (3) - three possibile outcomes
						- for Ω, for any binary process with equally probable outcomes,  Ω =2^n where n is number of outcomes of event 
							- Since in this case, n is the number of coin flips, n = 3 in this case, so 2^3 = 8 

2. Distributions in Machine Learning
	- Probability Distributions
		- Random Variables
			- Quantity that is produced by a random process
			- Random variable can take on one of many possible values
				- e.g. events from the state space
				- specific value or set of values for a random variable that can be assigned a probability
				- often denoted as capital letter (e.g. 'X')
					- values of the random variable are denoted as a lowercase letter and an index (e.g. x1, x2, x3)
			- Random Variables have Probability Distributions
				- In addition to calculating likelihood of events, can also be used to summarize likelihood of all possible outcomes
			- The summary of probabilities for the values of a random variable are called a probability distribution
				- Summarizes the relationship between possible values and their probability for a random variable
	- Continuous Probability Distributions
		- One category of probability distributions
		- Continuous Random Variable 
			- Has continuous probability distribution
			- Values are drawn from a range of real-valued numerical values
				- examples include adult height, infant weight, stock prices
			- Continuous probability Distribution describes the relationship between the events for a continuous random variable and their probabilities
			- The probability for a given continuous random variable cannot be specified directly
				- Probability CAN be specified for discrete random variables
				- For continuous random variable, probability is caluclated as an integral (area under the curve) for a tiny interval around a specific outcome
		- Uniform Distribution
			- Common form of continuous probability distribution
			- Uniform distribution refers to constant probabilities across the entire range of values in the domain which is the range of the x-axis for the random variable
				- found in dice rolls, card draws, model parameter for ml, radioactive particles, demand on the economy
			- (refer to colab notebook for this lesson)
				- in our randomly made plot, we see that distribution is mostly uniform (similar counts for values across scale) but is not exact, with some variables. Count is relatively similar for values.
		- Gaussian Distribution
			- Is also commonly called bell-curve distribution, or normal distribution
				- Very commonly used in stats and machine learning
			- Gaussian Distribution, for example, can determine probability of picking heads in coin flip
				- Can appear in sum of dice rolls, or height of people
				- another example is _ scores of pets
				- Covers probability of real-valued events
				- Continuous random variable that has normal distribution is considered normally distributed
			- (refer to colab notebook for this lesson)
		- Log-Normal distributions
			- Means that the natural logarithm of the log-normal distribution is normally distributed
				- examples include income, length of online comments, duration of chess games, duration of rubik's cube solves, size of video files, number of hospitalized cases in a pandemic
					- As income goes up, count of people with higher income decreases
					- Most comments on forums or videos are relatively short, and count of longer-form comments decreases as the number of words in a comment increases
			- in colab example, we see that the highest counts exist at the minimum, but as values increase, counts decrease, and decrease logarithmically 
				- IMPORTANT: Log-normal data is not itself normally distributed 
					- However, the LOG of the data is normally distributed, and we can see this when we create a normal plot of the log of our log-normal data (see colab) 				
		- Exponential Probability Distribution
			- few outcomes are the most likely
			- rapid decrease in probability to all other outcomes
			- Relatively squished up against zero and then decreases exponentially toward higher values
			- Examples include:
				- Time between requests to access Wikipedia pages
				- Time between clicks on a counter
				- Time until a part fails on a machine
				- Time until someone defaults on a loan 
			- Commonly seen in Deep Learning models
			- Equivalent to geometric probability distribution for discrete random variables
			- When you graph data that is exponentially distributed, it will look very similar to log-normal data, but if you plot the log of exponentially distributed data, it will not be normally distributed
		- Laplace Probability Distribution
			- Also called double exponential distribution
			- Commonly seen in extreme weather events such as maximum rainfall in a day
			- Has many ML applications, typically when an acute peak of probability is desired
			- when you plot data with a laplace distribution, it will take on a bell curve shape, but it is not normally distributed
				- in a laplace distribution, there will be discrepancies or jumps in values on the y-axis
		- Binomial Distribution
			- Binomial distribution is an example of a discrete probability distribution
			- Discrete random variables are variables drawn from a finite set of states
				- Heads/tails
				- is image dog or cat
				- true/false
			- True/false or 1/2 or 0/1 are booleans
			- y-axis of a charted boolean/discrete set shows event probability and x axis shows categories of outcomes
				- lets us see distribution of event probabilities
			- See colab notebook for example of probability testing for coin flips
			-Created by sampling bernoulli trials where N is greater than 1
			- Bernoulli distribution is equivalent of Binomial distribution where N of trials is 1
		- Multinomial Distribution
			- Generalization of Binomial Distribution to discrete random variables with more than 2 possible outcomes
				- Roll of a dice 
			- Works with any categorical variable 
		- Poisson Distribution
			- Used for count data (Number of cars drive by per minute, number of guests at a restaurant per day, number of new hires per month)
			- the lamba guides peak of distribution
				- sampling with lam 5 will tend to draw samples near 5
3. Machine Learning Optimization
	- In dataset, you have x/y 
		- Actual Dataset, have x random variable with y dependent variable
		- Model will predict y/yhat
		- Can quantify the difference between the 'correct' target model output y and the model's predicted output yhat
		- Error = yhat-y
		- Typically consider several instances simultaneously
		- simple approach
			- Take average of all errors for each samples
				-eg) errors 5,3
					-Average error = (5+3)/2 = 4 - 4 is average error of model
					- Problem with this is that you may have negative numbers as errors (eg 5, -5)
						-Average error = (5 + (-5))/2 = 0
							- This is bad because it implies that the model is perfect (it is not)
								- To resolve this, use the absolute value of the error instead (Mean Absolute Error)
									- eg) error 5, -5
										- Average Error = (|5| + |-5|)/2 = 5
						- Mean squared error 
							- relatively tolerant of smaller errors, but less tolerant of larger errors, which tends to lead to better-fitting ML models